{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srXC6pLGLwS6"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKJgh75dF4vk",
        "outputId": "ae162ee2-0cf5-43ef-a2cd-bf40aaa94cc6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Para importar de drive:\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "'''\n",
        "\n",
        "path_to_directory = \" _ \"\n",
        "file_name = \" _ \"\n",
        "path_to_file = str(path_to_directory+file_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGyKZj3bzf9p"
      },
      "source": [
        "### Import TensorFlow and other libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yG_n40gFzf9s"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pD_55cOxLkAb",
        "outputId": "dc64edde-8553-43ff-8607-c0a70b52f454"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 1 files belonging to 1 classes.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<_BatchDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Importamos el dataset con keras\n",
        "tf.keras.utils.text_dataset_from_directory(\n",
        "    path_to_directory,\n",
        "    labels=\"inferred\",\n",
        "    label_mode=\"int\",\n",
        "    class_names=None,\n",
        "    batch_size=32,\n",
        "    max_length=None,\n",
        "    shuffle=True,\n",
        "    seed=None,\n",
        "    validation_split=None,\n",
        "    subset=None,\n",
        "    follow_links=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHjdCjDuSvX_"
      },
      "source": [
        "### Read the data\n",
        "\n",
        "First, look in the text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aavnuByVymwK",
        "outputId": "01104c54-3d05-4a2c-f8ef-990da3c7cee6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length of text: 673492 characters\n",
            "It is a truth universally acknowledged, that a single man in possession\n",
            "of a good fortune must be in want of a wife.\n",
            "80 unique characters\n"
          ]
        }
      ],
      "source": [
        "# Leer el .txt\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# Tamaño del alfabeto\n",
        "print(f'Length of text: {len(text)} characters')\n",
        "# Ve primeras lineas\n",
        "print(text[:116])\n",
        "# Alfabeto\n",
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IlCgQBRVymwR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNnrKn_lL-IJ"
      },
      "source": [
        "## Process the text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6GMlCe3qzaL9"
      },
      "outputs": [],
      "source": [
        "# Asigna uid numericos unicos a los caracteres para poder trabajar con ellos\n",
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "    vocabulary=list(vocab), mask_token=None)\n",
        "# Inverso al anterior, recupera los caracteres desde los uid\n",
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)\n",
        "# Esto \"destokeniza\" y vuelve a unir los caracteres\n",
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UopbsKi88tm5",
        "outputId": "8acacbc8-0b60-478b-dbf7-579531f67dbe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(673492,), dtype=int64, numpy=array([28, 66,  2, ..., 65,  9, 80])>"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Tamaño del archivo en caracteres y el texto codificado caracteres a uid\n",
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qmxrYDCTy-eL"
      },
      "outputs": [],
      "source": [
        "# El texto pero traducidos los caracteres a los ids\n",
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cjH5v45-yqqH",
        "outputId": "c60b505b-1e7e-4dcf-d5af-9a9dffb0edc3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It is a truth universally acknowledged, that a single man in possession\n",
            "of a good fortune must be in want of a wife."
          ]
        }
      ],
      "source": [
        "# Recupera el id a partir de las listas de IDs y los traduce con chars_from_ids a caracteres legibles\n",
        "# Recupera y traduce la primera frase de Pride_and_Prejudice.txt\n",
        "first_sentence = []\n",
        "for ids in ids_dataset.take(116):\n",
        "    first_sentence.append(chars_from_ids(ids).numpy().decode('utf-8'))\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'), end= \"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-G2oaTxy6km"
      },
      "outputs": [],
      "source": [
        "# Tamaño de las secuencias de caracteres que vamos a usar\n",
        "seq_length = 100\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZSYAcQV8OGP"
      },
      "source": [
        "The `batch` method lets you easily convert these individual characters to sequences of the desired size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BpdjRO2CzOfZ",
        "outputId": "d4acab59-289a-47a3-bb96-c859e06e40ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[b'I' b't' b' ' b'i' b's' b' ' b'a' b' ' b't' b'r' b'u' b't' b'h' b' '\n",
            " b'u' b'n' b'i' b'v' b'e' b'r' b's' b'a' b'l' b'l' b'y' b' ' b'a' b'c'\n",
            " b'k' b'n' b'o' b'w' b'l' b'e' b'd' b'g' b'e' b'd' b',' b' ' b't' b'h'\n",
            " b'a' b't' b' ' b'a' b' ' b's' b'i' b'n' b'g' b'l' b'e' b' ' b'm' b'a'\n",
            " b'n' b' ' b'i' b'n' b' ' b'p' b'o' b's' b's' b'e' b's' b's' b'i' b'o'\n",
            " b'n' b'\\n' b'o' b'f' b' ' b'a' b' ' b'g' b'o' b'o' b'd' b' ' b'f' b'o'\n",
            " b'r' b't' b'u' b'n' b'e' b' ' b'm' b'u' b's' b't' b' ' b'b' b'e' b' '\n",
            " b'i' b'n' b' '], shape=(101,), dtype=string)\n"
          ]
        }
      ],
      "source": [
        "# Divide el dataset en batches de la longitud introducida +1\n",
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))\n",
        "# Esto lo imprime feo porque no está hecoh el join"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PHW902-4oZt"
      },
      "source": [
        "It's easier to see what this is doing if you join the tokens back into strings:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QO32cMWu4a06",
        "outputId": "d90a6090-27a9-458a-fef1-96def0e0594e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "b'It is a truth universally acknowledged, that a single man in possession\\nof a good fortune must be in '\n",
            "b'want of a wife.\\n\\nHowever little known the feelings or views of such a man may be on his\\nfirst enterin'\n",
            "b'g a neighbourhood, this truth is so well fixed in the minds\\nof the surrounding families, that he is c'\n"
          ]
        }
      ],
      "source": [
        "for seq in sequences.take(3):\n",
        "  print(text_from_ids(seq).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbLcIPBj_mWZ"
      },
      "source": [
        "For training you'll need a dataset of `(input, label)` pairs. Where `input` and\n",
        "`label` are sequences. At each time step the input is the current character and the label is the next character.\n",
        "\n",
        "Here's a function that takes a sequence as input, duplicates, and shifts it to align the input and label for each timestep:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9NGu-FkO_kYU",
        "outputId": "2dab39eb-1a8b-4dd8-be03-b68ef8305d61"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(['I', 't', ' ', 'i', 's', ' ', 'a', ' ', 't', 'r', 'u', 't', 'h', ' '],\n",
              " ['t', ' ', 'i', 's', ' ', 'a', ' ', 't', 'r', 'u', 't', 'h', ' ', 'u'])"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "def split_input_target(sequence):\n",
        "    \"\"\"\n",
        "    necesitamos pares input,label para predecir,\n",
        "    input es el caracter actual y label el siguiente\n",
        "    aqui duplicamos la secuencia y la desplazamos una posicion para hacer coincidir input con label\n",
        "    (por eso antes habiamos puesto el +1 en la longitud)\n",
        "\n",
        "    \"\"\"\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "# Ejemplo\n",
        "split_input_target(first_sentence[:15])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9iKPXkw5xwa",
        "outputId": "c3481046-7faf-4dd9-b274-f38900fc29e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input : b'It is a truth universally acknowledged, that a single man in possession\\nof a good fortune must be in'\n",
            "Target: b't is a truth universally acknowledged, that a single man in possession\\nof a good fortune must be in '\n"
          ]
        }
      ],
      "source": [
        "# Este será el dataset con el que trabajemos, parejas (sequence, sequence desplazado a la derecha una posicion)\n",
        "dataset = sequences.map(split_input_target)\n",
        "# Ejemplo\n",
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2pGotuNzf-S",
        "outputId": "0198466d-0bbe-45a3-c443-4f027ba0e001"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(120, 100), dtype=tf.int64, name=None), TensorSpec(shape=(120, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Tamaño del batch para entrenamiento\n",
        "BATCH_SIZE = 120\n",
        "\n",
        "# Para barajar(shuffle)\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6oUuElIMgVx"
      },
      "source": [
        "## Build The Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHT8cLh7EAsg"
      },
      "outputs": [],
      "source": [
        "# Longitud del alfabeto\n",
        "vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "\n",
        "# Tamaño del vector que dará el embedding\n",
        "embedding_dim = 256\n",
        "\n",
        "# Neuronas en la capa RNN, con este dataset funciona bien a partir de 700/1000\n",
        "rnn_units = 2000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wj8HQ2w8z4iO"
      },
      "outputs": [],
      "source": [
        "# Defino la red neuronal\n",
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    # Capa de entrada con el tamaño de embedding definido anteriormente\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    # La capa de RNN\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   # Devuelve toda la sequence, no solo el ultimo caracter. Este es el output propiamente dicho\n",
        "                                   return_sequences=True,\n",
        "                                   # Devuelve el state (pesos actuales) además del output\n",
        "                                   return_state=True)\n",
        "\n",
        "    #capa de salida, con activación lineal y tiene vocab_size neuronas, una por caracter que puede ser predicho\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  # Llama la red neuronal sobre un input y devuelve la transformacion\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IX58Xj9z47Aw"
      },
      "outputs": [],
      "source": [
        "# Inicializo el objeto model a partir de la clase anterior\n",
        "model = MyModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-_70kKAPrPU",
        "outputId": "de605b0f-e0e9-4123-e91f-ca0a70289d9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(120, 100, 81) (120, 100, 81)\n"
          ]
        }
      ],
      "source": [
        "  # Tamaño del batch, longitud de la secuencia, tamaño del vocabulario\n",
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape,  (BATCH_SIZE, seq_length, vocab_size))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPGmAAXmVLGC",
        "outputId": "02150b99-1059-453e-ffbd-f49ccf6c25c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  20736     \n",
            "                                                                 \n",
            " gru (GRU)                   multiple                  13548000  \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  162081    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 13730817 (52.38 MB)\n",
            "Trainable params: 13730817 (52.38 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZOeWdgxNFDXq"
      },
      "outputs": [],
      "source": [
        "# Funcion de pérdida, logits mapea [0,1]--->[-inf,+inf] con nna logaritmica\n",
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "# Lo compilo y le añado un optimizador (compile prepara el modelo para entrenamiento)\n",
        "model.compile(optimizer='adam', loss=loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7yGBE2zxMMHs"
      },
      "outputs": [],
      "source": [
        "# Número de épocas para entrenar, entre 20 y 30 funciona mejor\n",
        "EPOCHS = 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UK-hmKjYVoll",
        "outputId": "fc3995cc-4ed2-47ac-b292-b64559252397"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "55/55 - 21s - loss: 1.8875 - 21s/epoch - 374ms/step\n",
            "Epoch 2/30\n",
            "55/55 - 23s - loss: 1.6869 - 23s/epoch - 422ms/step\n",
            "Epoch 3/30\n",
            "55/55 - 22s - loss: 1.5263 - 22s/epoch - 401ms/step\n",
            "Epoch 4/30\n",
            "55/55 - 21s - loss: 1.4004 - 21s/epoch - 375ms/step\n",
            "Epoch 5/30\n",
            "55/55 - 21s - loss: 1.3076 - 21s/epoch - 379ms/step\n",
            "Epoch 6/30\n",
            "55/55 - 21s - loss: 1.2349 - 21s/epoch - 383ms/step\n",
            "Epoch 7/30\n",
            "55/55 - 21s - loss: 1.1727 - 21s/epoch - 385ms/step\n",
            "Epoch 8/30\n",
            "55/55 - 21s - loss: 1.1145 - 21s/epoch - 387ms/step\n",
            "Epoch 9/30\n",
            "55/55 - 21s - loss: 1.0640 - 21s/epoch - 378ms/step\n",
            "Epoch 10/30\n",
            "55/55 - 22s - loss: 1.0165 - 22s/epoch - 399ms/step\n",
            "Epoch 11/30\n",
            "55/55 - 21s - loss: 0.9645 - 21s/epoch - 382ms/step\n",
            "Epoch 12/30\n",
            "55/55 - 21s - loss: 0.9126 - 21s/epoch - 388ms/step\n",
            "Epoch 13/30\n",
            "55/55 - 23s - loss: 0.8569 - 23s/epoch - 411ms/step\n",
            "Epoch 14/30\n",
            "55/55 - 21s - loss: 0.7949 - 21s/epoch - 378ms/step\n",
            "Epoch 15/30\n",
            "55/55 - 22s - loss: 0.7304 - 22s/epoch - 401ms/step\n",
            "Epoch 16/30\n",
            "55/55 - 21s - loss: 0.6619 - 21s/epoch - 385ms/step\n",
            "Epoch 17/30\n",
            "55/55 - 21s - loss: 0.5870 - 21s/epoch - 385ms/step\n",
            "Epoch 18/30\n",
            "55/55 - 21s - loss: 0.5131 - 21s/epoch - 377ms/step\n",
            "Epoch 19/30\n",
            "55/55 - 21s - loss: 0.4410 - 21s/epoch - 385ms/step\n",
            "Epoch 20/30\n",
            "55/55 - 21s - loss: 0.3722 - 21s/epoch - 389ms/step\n",
            "Epoch 21/30\n",
            "55/55 - 22s - loss: 0.3113 - 22s/epoch - 393ms/step\n",
            "Epoch 22/30\n",
            "55/55 - 22s - loss: 0.2569 - 22s/epoch - 398ms/step\n",
            "Epoch 23/30\n",
            "55/55 - 21s - loss: 0.2138 - 21s/epoch - 379ms/step\n",
            "Epoch 24/30\n",
            "55/55 - 22s - loss: 0.1796 - 22s/epoch - 408ms/step\n",
            "Epoch 25/30\n",
            "55/55 - 21s - loss: 0.1533 - 21s/epoch - 385ms/step\n",
            "Epoch 26/30\n",
            "55/55 - 21s - loss: 0.1322 - 21s/epoch - 380ms/step\n",
            "Epoch 27/30\n",
            "55/55 - 21s - loss: 0.1175 - 21s/epoch - 383ms/step\n",
            "Epoch 28/30\n",
            "55/55 - 22s - loss: 0.1076 - 22s/epoch - 408ms/step\n",
            "Epoch 29/30\n",
            "55/55 - 22s - loss: 0.1006 - 22s/epoch - 408ms/step\n",
            "Epoch 30/30\n",
            "55/55 - 22s - loss: 0.0955 - 22s/epoch - 392ms/step\n"
          ]
        }
      ],
      "source": [
        "# Entrenamos el modelo\n",
        "history = model.fit(dataset, epochs=EPOCHS,verbose= 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iSBU1tHmlUSs"
      },
      "outputs": [],
      "source": [
        "# Defino la clase OneStep para empaquetar el modelo\n",
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=.3):\n",
        "    # Temperature influye en la aleatoriedad del modelo\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Para eliminar characteres incorrectos.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Encaja shape con el alfabeto para poder encadenar predicciones\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "  # Crea un grafo de tf para optimizar\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Tokeniza en IDS\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Ejecuta el modelo y coge la ultima prediccion\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    # Aplica la temperatura\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Aplica la mascara para eliminar caracteres incorrectos\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Elige logits y genera UIDs\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # UID a caracter legible\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Devuelve caracteres y el estado del modelo\n",
        "    return predicted_chars, states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqMOuDutnOxK"
      },
      "outputs": [],
      "source": [
        "# Para aplicar el modelo\n",
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9yDoa0G3IgQ"
      },
      "source": [
        "Run it in a loop to generate some text. Looking at the generated text, you'll see the model knows when to capitalize, make paragraphs and imitates a Shakespeare-like writing vocabulary. With the small number of training epochs, it has not yet learned to form coherent sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ST7PSyk9t1mT",
        "outputId": "4658e2a3-157d-42c0-e353-90dd0a55cabb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "________________________________________________________________________________ \n",
            "\n",
            " Elizabeth looked archly, and turned away. Her resistance had not injured\n",
            "her with the gentleman, and the only face\n",
            "whose felt in the power of choice. I do not know anybody who seems\n",
            "more to enjoy them in a desirable match for Jane,” said she, “I should not care about it myself; but the\n",
            "grounds are delightful. They have at least knew that she\n",
            "could not speak a word, especially to Miss Darcy, who had been concerned in the measures\n",
            "taken to separate Mr. Bingley on the enjoyment of it had been\n",
            "little. Eager to be alone, and fearful of its being the most\n",
            "remarkable charm of the evening, and might now come to inquire particularly after her. But\n",
            "this information, the misery she would be serious, however, to increase her vexations by dwelling on them.\n",
            "She was confident of her being presentage to her feelings, capable of\n",
            "conversation, which had been pleased to find that she\n",
            "had satisfied the door. He then sat down by her, and taken place, my dear cousin, that your\n",
            "refusal of my addresses are merely word \n",
            "\n",
            "________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Inicializo el estado\n",
        "states = None\n",
        "# Inicializo el texto (prompt)\n",
        "next_char = tf.constant(['Elizabeth'])\n",
        "# Predicción\n",
        "result = [next_char]\n",
        "\n",
        "# Genera un texto de n caracteres con el modelo entrenadi y el prompt\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "\n",
        "#\n",
        "print('_'*80,'\\n\\n',result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Grk32H_CzsC",
        "outputId": "4a7a234b-7fa6-41ab-c84a-09678750f440"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7b6fc0263760>, because it is not built.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
          ]
        }
      ],
      "source": [
        "# Guardar el modelo\n",
        "tf.saved_model.save(one_step_model, 'one_step')\n",
        "one_step_reloaded = tf.saved_model.load('one_step')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "id": "ahV8jmlGlh-R",
        "outputId": "3dfff756-0986-44a5-e3a2-31014ec89f65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "updating: content/one_step/ (stored 0%)\n",
            "updating: content/one_step/saved_model.pb (deflated 90%)\n",
            "updating: content/one_step/variables/ (stored 0%)\n",
            "updating: content/one_step/variables/variables.data-00000-of-00001 (deflated 7%)\n",
            "updating: content/one_step/variables/variables.index (deflated 58%)\n",
            "updating: content/one_step/assets/ (stored 0%)\n",
            "updating: content/one_step/fingerprint.pb (stored 0%)\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_2a8e63b7-a4c4-46ce-aa77-fe06edf4b2f3\", \"Trained_Austen.zip\", 152508664)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Descargar el modelo\n",
        "nombre = \"nombre_del_modelo\"\n",
        "model_name = \"Trained_\" + nombre + \".zip\"\n",
        "!zip -r model_name /content/one_step\n",
        "from google.colab import files\n",
        "files.downloadmodel_name)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
